
<choose the word:(performance vs fitness)>

The bake-off <ref> setup for the experimental parameter evalution<ref>:

Evaluated parameters:
\begin{description}
    \item[p] The norm (where)
    \item[a] Penelty for the norm
    \item[N] Number of transitions
    \item[cp] Particle count
    \item[$\sigma$] Resample blur (?)
\end{description}

will be tested on a small set of benchmarks consisting of 4 disperse(d?)/scatter generated whisker videos.

The evaluation tensor:
\begin{equation}
    \Phi_{p,a,N,...}(benchmark)
\end{equation}

TODO (check the comments here)
%TODO define phi:img->img
%TODO define \phi(R(x))*\phi(I) as <x,I>_\phi

The measures that will be used as fitness of the parameters:
    \begin{description}
        \item[$\int{||\epsilon(t)||_{L^p}}dt$]
            integrating over time the the difference
            with the ground truth (do this for L{1:10} and see if it correlates with
            the p choosed (to see how much it deviates from the ground truth)
        \item[$\int{\sum{\phi{R(x_t)}\phi{I_t}} }dt$] 
            integrating over time the response
            for the choosed hypotethis (to see how the different image transformation
            affects the results, that is if it only follows what it thinks is best
            (phi))
        \item[Subjective] 4 image samples
    \end{description}
And all this are done for all 4 benchmark videos.


"There are many metrics by which a model may be assessed." - Encyclopedia


The fitness test was runned on different machines but this wont effect the
result since we initially wont consider the running time.

The runtime for the algorithm is handled separately on one machine setup <...>



Tillvägagångsätt:
1. Since we have prior knowledge about the effect off varying the parameters cp,N
they will firstly be set to a sufficently large value.
2. A partion of the test-matrix will then be evaluated 
