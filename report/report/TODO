
Classify our algorithm (lazy/eager) see bik12

rename goodness to "closeness" since its more widely used in ML

Minkowski metric - L_p norm (L_1(more resistant to noise) and L_inf(noisy))
Our distance functions must be metric, prove this for all.

Feature selection: (100D noise,1D disc. information) - with unweighted euclidian distance the 100D will always dominate \sum(x_i-mu_i)^2
solution: We want large variation in the class labels to be more significant (for example, PCA assumes label variation is proportional to variation, which is some times generally true)

test different combinations of algorithms fitnesses=lambda (func1,func2,...) :[(func1,func2,..),fitness(func1,func2,...)], prod(FuncSpace1,FuncSpace2,...)
best = max(fitnesses)
where fitness is how well the algorithm performes on validatiot-data

try the methods for kNN speedup in our wdb
bucketing
KD-tree

bayes optimal as in bik12, is this applicable ?


Can something be considered to be an heuristic?

============================================
Seperate filter (as in image filter) and filter (as in tracker)


==========================================
PF vs condensation


==========================================
(not even considering)Non adaptive [ref:probrob] since we dont have realtime issues


===========================================

Prove particle approx for PDF's (lim analysis?) and show this by example, also show the PDF-multiplication we are doing in the PF with example (and proof?)



===================== PF as searchblablabla =================

PF is simply a directed search

Searching in this m-dimensional featurespace can be done in O(log(n^m))=O(mlog(n)) but filling it adequately still requires O(n^m)

Considering configurations of the model that are improbable is totally unnecessary<image of whale and flower>


============= Problems ========================


Artifacts from encoding the video.

Whiskers are mostly subpixel objects.

Images are a sampling of a continous (disregarding quantum effects) view of the reality
img=\sum\dirac{}

=============== Goal =====================

(Mockup?) Standalone application




bik12-holistic appearance-based descriptor



"partikelfiltrering: approximativ slutledning i dynamiska bayesianska nätverk"
Point out the connection between physical kinematic system (present the main formulaes that governs the motions of objects and show that they are infact a dynamic system, then adding uncertaintis we get bayesian network)


Borrow the word reduce from complexity theory and use it as: reducing the problem to an innerstate of an dynamicsystem



================= Own Contributions =============
<<Vart ska dethär in?>>


Given PF a model of whiskers ...



================== Scope ==================
<<>>

Since the video-sequences we are intending to run on is grayscale-only we simply 
transform all the images down to grayscale which wont affect the results even if
the image happens to be in color.




========================================

Feature extraction vs. feature selection (bik)





============= The handling of uniform (seen from prop.dist in featurespace) like base-position/angle ===================






"In its rawest form"


=============== WRAPUP ===================

(Mockup?)

Having the goal to make it an standalone application had some challanges 
compared to do a simple muckup in MATLAB(R) or such since some builtin
functionallity is missing and also having problems like missaligned buffers 
or having to write an adapter between images and matrices as an example.




==========================================
Do an exhausted search for something low dimensional like the snot and then plot all the datapoints over time for a sequence in the video.
Will be very high dimensional so perhaps some projection will be good but keep time as an axis
The goal is to see the "worm" lingering tru the timeline (is it possible to do an variant of density-based clustering of this data to first find the path tru time of these worms given an head of the worm (a startpoint at t_0) and then "tracking" the timeworm? and ofcourse trying to find E[] of the timeworm)



================Analysis====================
can one just do analysis with a cost function, like cost=compution and to maximize accuracy/computations by using an directed search, even so much that we basically lowers the complexity of the algorithm
Is it possible to do somewhat of of complexity analysis of this?
Worstcase data? better data (assumptions in realdata?)
without directed search (AKA uniform distribution pick) O(n^k)
searching in an gaussianclock? searching in an flat gaussian clock, that is in an gaussian clock that has perfekt dependacies in some direction (that is no (or almost no) data in that dimension)



==========================================
Bra grejer att referera till i encylopdian:
>Accuracy, "refers to a measure of the degree to which the predictions of a model match the reality being modeled.
>Passive learning (@Active Learning), where the learner is simply presentedwith a training set over which it has no control.
>Statiscical Active Learning (under active learning), can our thing classify as this perhaps?
tex. "Structure of learning system"-dyker upp som delkapitel, ska vi ha samma struktur som i encyclopedian ?
"Learning system" (which part are the learnigsystem in our case?)
"Winner-Take-All"
>Algorithm Evalation (follow the model when evaluating our algorithm)
>Attribute (synonymsd: <<feature>>,property,trait,characteristic)
>Bake-Off, "Bake-off is a disparaging term for experimental eval-
uation of multiple learning algorithms by a process of
applying each algorithm to a limited set of benchmark
problems." (crossreference:algorithm evalution)
>Bayes Rule
>Bias
>Confusion matrix (realclass vs assigned class) just relate it to if it fails and add risk (indicator if the algorithm starts to slip in the tracking)
>Cost Function (do we apply it somewhere?), can one just do analysis with a cost function, like cost=compution and to maximize accuracy/computations by using an directed search.
>Covariance
>Cross-Validation (is this possible to implement, instead of doing the trivial test/train?)
diagram at p116 (C.pdf) "system dimension" classify the algorithm diagram
>>Curse of Dimensionality   
>Data preperation (data preprocessing)
>Data set (for example we have 2 data sets, a training set which is the input for the learning system for it to analyze and learn a model and a test set which is used to evaluate the model learned by the system.
>>Density Estimation "given a set of observations, which is a random sample from a probability desntiy function f_X(x) desnity estimation attempts to approximate f by fhat, a simple method ..." (also see crossreference)
>Discretization (be carefull with the use of this in the report) [[Interesting enlisting: eager/lazy global/local etcetc]]
>Distance measure (simliarity measure)
>(Exploration, searching in search space)
>Expected Maximization
>Epsilon cover (all points e-close to a subset)
>Error rate
>Error Squared "Error squared is a common loss function used with
regression. This is the square of the difference between
the predicted and true values."
>Evaluation / Evaluation data
>("Expectation propagation is an algorithm for Bayesian
=machine learning (see Bayesian Methods). It tunes the parameters of a simpler approximate distribution (e.g., a Gaussian) to match the exact posterior distribution of the model parameters given the data. ") (readmore) (In kalman (and variants) vs. partikelfilter where you dont need to know the distribution)
>Expacted maximization
>Explanation-Based Learning. Figure . Conventional learner
>Experience curce(learning curves in machine learning)
>Feature extraction/selection (dimensionality reduction basically) (compare clean selection with the probability selection, that is proportional to how important it is, the feature isnt really important to search in if you already can predict it with great certainty)
>

==Lookup words from above(are these relevant?)==
>Hypotesis Language (under ANN.crossreference)
>Semi supervised learning (Co-training)
>Gaussian processes
>Similarity measure
>ROC analysis
>Learning Curves in Machine learning

Bonuses:





===========================================
===========================================
According to "Att genomföra projektarbeten"



