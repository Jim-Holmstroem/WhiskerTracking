
Classify our algorithm (lazy/eager) see bik12

rename goodness to "closeness" since its more widely used in ML

Minkowski metric - L_p norm (L_1(more resistant to noise) and L_inf(noisy))
Our distance functions must be metric, prove this for all.

Feature selection: (100D noise,1D disc. information) - with unweighted euclidian distance the 100D will always dominate \sum(x_i-mu_i)^2
solution: We want large variation in the class labels to be more significant (for example, PCA assumes label variation is proportional to variation, which is some times generally true)

test different combinations of algorithms fitnesses=lambda (func1,func2,...) :[(func1,func2,..),fitness(func1,func2,...)], prod(FuncSpace1,FuncSpace2,...)
best = max(fitnesses)
where fitness is how well the algorithm performes on validatiot-data

try the methods for kNN speedup in our wdb
bucketing
KD-tree

bayes optimal as in bik12, is this applicable ?


Can something be considered to be an heuristic?

============================================
Seperate filter (as in image filter) and filter (as in tracker)


==========================================
PF vs condensation


==========================================
(not even considering)Non adaptive [ref:probrob] since we dont have realtime issues


===========================================

Prove particle approx for PDF's (lim analysis?) and show this by example, also show the PDF-multiplication we are doing in the PF with example (and proof?)



===================== PF as searchblablabla =================

PF is simply a directed search

Searching in this m-dimensional featurespace can be done in O(log(n^m))=O(mlog(n)) but filling it adequately still requires O(n^m)

Considering configurations of the model that are improbable is totally unnecessary<image of whale and flower>


============= Problems ========================


Artifacts from encoding the video.

Whiskers are mostly subpixel objects.

Images are a sampling of a continous (disregarding quantum effects) view of the reality
img=\sum\dirac{}

=============== Goal =====================

(Mockup?) Standalone application




bik12-holistic appearance-based descriptor



"partikelfiltrering: approximativ slutledning i dynamiska bayesianska nätverk"
Point out the connection between physical kinematic system (present the main formulaes that governs the motions of objects and show that they are infact a dynamic system, then adding uncertaintis we get bayesian network)


Borrow the word reduce from complexity theory and use it as: reducing the problem to an innerstate of an dynamicsystem



================= Own Contributions =============
<<Vart ska dethär in?>>


Given PF a model of whiskers ...



================== Scope ==================
<<>>

Since the video-sequences we are intending to run on is grayscale-only we simply 
transform all the images down to grayscale which wont affect the results even if
the image happens to be in color.




========================================

Feature extraction vs. feature selection (bik)





============= The handling of uniform (seen from prop.dist in featurespace) like base-position/angle ===================






"In its rawest form"


=============== WRAPUP ===================

(Mockup?)

Having the goal to make it an standalone application had some challanges 
compared to do a simple muckup in MATLAB(R) or such since some builtin
functionallity is missing and also having problems like missaligned buffers 
or having to write an adapter between images and matrices as an example.



================Analysis====================
can one just do analysis with a cost function, like cost=compution and to maximize accuracy/computations by using an directed search, even so much that we basically lowers the complexity of the algorithm
Is it possible to do somewhat of of complexity analysis of this?
Worstcase data? better data (assumptions in realdata?)
without directed search (AKA uniform distribution pick) O(n^k)
searching in an gaussianclock? searching in an flat gaussian clock, that is in an gaussian clock that has perfekt dependacies in some direction (that is no (or almost no) data in that dimension)



==========================================
Bra grejer att referera till i encylopdian:
>Accuracy, "refers to a measure of the degree to which the predictions of a model match the reality being modeled.
>Passive learning (@Active Learning), where the learner is simply presentedwith a training set over which it has no control.
>Statiscical Active Learning (under active learning), can our thing classify as this perhaps?
tex. "Structure of learning system"-dyker upp som delkapitel, ska vi ha samma struktur som i encyclopedian ?
"Learning system" (which part are the learnigsystem in our case?)
"Winner-Take-All"
>Algorithm Evalation (follow the model when evaluating our algorithm)
>Attribute (synonymsd: <<feature>>,property,trait,characteristic)
>Bake-Off, "Bake-off is a disparaging term for experimental eval-
uation of multiple learning algorithms by a process of
applying each algorithm to a limited set of benchmark
problems." (crossreference:algorithm evalution)
>Bayes Rule
>Bias
>Confusion matrix (realclass vs assigned class) just relate it to if it fails and add risk (indicator if the algorithm starts to slip in the tracking)
>Cost Function (do we apply it somewhere?), can one just do analysis with a cost function, like cost=compution and to maximize accuracy/computations by using an directed search.
>Covariance
>Cross-Validation (is this possible to implement, instead of doing the trivial test/train?)
diagram at p116 (C.pdf) "system dimension" classify the algorithm diagram
>>Curse of Dimensionality   

[@D.pdf]


==Lookup words from above(are these relevant?)==
>Hypotesis Language (under ANN.crossreference)
>Semi supervised learning (Co-training)
>Gaussian processes


===========================================
===========================================
According to "Att genomföra projektarbeten"



