

Classify our algorithm (lazy/eager) see bik12

rename goodness to "closeness" since its more widely used in ML

Minkowski metric - L_p norm (L_1(more resistant to noise) and L_inf(noisy))
Our distance functions must be metric, prove this for all.

Feature selection: (100D noise,1D disc. information) - with unweighted euclidian distance the 100D will always dominate \sum(x_i-mu_i)^2
solution: We want large variation in the class labels to be more significant (for example, PCA assumes label variation is proportional to variation, which is some times generally true)

test different combinations of algorithms fitnesses=lambda (func1,func2,...) :[(func1,func2,..),fitness(func1,func2,...)], prod(FuncSpace1,FuncSpace2,...)
best = max(fitnesses)
where fitness is how well the algorithm performes on validatiot-data

try the methods for kNN speedup in our wdb
bucketing
KD-tree

bayes optimal as in bik12, is this applicable ?




Add to report:

Explain: Hypotesis, sample, DOF?, 

==Curse of dimensinallity
{problem}
The 8 DOF in our model produces vast amounts of space in our featurespace and this has the consequence that we need \proportianal n^8 datapoints to fill it to an specified density c.
As an example, we would need 4^8=65536 samples just to make a grid with 4 samples in each DOF which in many cases (aspecially for hand labeled data). 
{classification}
This phenomen of having hue searchspace in highdimensinonal space is fairly common and have the name "Curse of dimensionallity" http://en.wikipedia.org/wiki/Curse_of_dimensionality
{solution}
One way to somewhat overcome this emptyness in space is to have a dynamic (active?) algorithm that adopts the sample density 
according to the models relative frequency in that area and this results in, for a given amount of samples, its more likely 
for an sampled model to occure in a more densly pre-sampled (pre-sampled?) area, in fact this method when doing it right (ideally) 
gives: given a set of samples the overall density for all the samples in the sampledatabase would be optimal) [proof for this will be given in <blabla>]
... (use the world hypotesis instead of sample, or perhaps sampled-hypotesis)
(below is perhaps somewhat redudant, but one can pick bits and pieces from both)
So having a bias towards trying out more plausible hypotesis for the models innerstate is better than 
doing a naive exhausted search tru the entire featurespace, this could only be used if you have \le 3 DOF as in for example finding straight edges with houghtransformation[ref].
...
One method that uses prior knowledge of the models PDF and a pretrained database containing knowledge on how to approximate the models PDF in the next timestep is particle filter which is an (instance?) of the ideal bayesian filter.




Searching in this m-dimensional featurespace can be done in O(log(n^m))=O(mlog(n)) but filling it addicitly still requeres O(n^m)


Considering configurations of the model that are impropable is totally unncesasairy<image of whale and flower>

Borrow the word reduce from complexity theory and use it as: reducing the problem to an innerstate of an dynamicsystem
